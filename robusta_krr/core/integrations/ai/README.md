# AI-Assisted Strategy Implementation

This directory contains the implementation of the AI-Assisted resource recommendation strategy for KRR.

## Architecture

```
robusta_krr/
â”œâ”€â”€ core/integrations/ai/
â”‚   â”œâ”€â”€ __init__.py              # Provider factory
â”‚   â”œâ”€â”€ base.py                  # Abstract AIProvider base class
â”‚   â”œâ”€â”€ openai_provider.py       # OpenAI implementation
â”‚   â”œâ”€â”€ gemini_provider.py       # Google Gemini implementation
â”‚   â”œâ”€â”€ anthropic_provider.py    # Anthropic Claude implementation
â”‚   â””â”€â”€ ollama_provider.py       # Ollama local implementation
â””â”€â”€ strategies/
    â”œâ”€â”€ ai_assisted.py           # Main AI strategy
    â””â”€â”€ ai_prompts.py            # Prompt generation & stats extraction
```

## Components

### 1. AI Providers (`core/integrations/ai/`)

**Base Provider (`base.py`):**
- Abstract class defining the AI provider interface
- Retry logic with exponential backoff (3 attempts)
- JSON extraction with regex fallback for markdown-wrapped responses
- HTTP request handling using `requests` library

**Provider Implementations:**
- **OpenAI** (`openai_provider.py`): GPT-4, GPT-3.5-turbo, etc.
- **Gemini** (`gemini_provider.py`): Gemini Pro, Gemini 1.5 Pro
- **Anthropic** (`anthropic_provider.py`): Claude 3 Opus/Sonnet/Haiku
- **Ollama** (`ollama_provider.py`): Local models (Llama 3, Mistral, etc.)

Each provider implements:
```python
def _get_endpoint(self) -> str
def _get_headers(self) -> dict
def _format_request_body(self, messages: Union[list, str]) -> dict
def _parse_response(self, response_data: dict) -> str
```

### 2. AI Strategy (`strategies/ai_assisted.py`)

**Main Components:**
- `AiAssistedStrategySettings`: Pydantic settings model with 12 AI-specific fields
- `AiAssistedStrategy`: Strategy implementation extending `BaseStrategy`
- Auto-detection of AI provider from environment variables
- Validation and sanity checking against Simple strategy
- Min/max constraint enforcement

**Key Methods:**
- `_detect_provider()`: Auto-detect provider from env vars
- `run()`: Main execution logic
- `_sanity_check()`: Compare against Simple strategy

### 3. Prompt Generation (`strategies/ai_prompts.py`)

**Functions:**
- `extract_comprehensive_stats()`: Extract CPU/Memory statistics from Prometheus data
- `get_system_prompt()`: Generate AI instruction prompt with JSON schema
- `get_user_prompt()`: Format workload statistics (full/compact modes)
- `format_messages()`: Provider-specific message formatting

**Statistics Extracted:**
- CPU: Percentiles (p50, p75, p90, p95, p99), mean, std, trend slope, spike count
- Memory: Max, mean, std, per-pod breakdown, OOMKill detection
- Context: HPA configuration, current allocations, warnings

## Features

### âœ… Implemented

1. **Multi-Provider Support**: OpenAI, Gemini, Anthropic, Ollama
2. **Auto-Detection**: Automatically detect provider from environment variables
3. **Compact Mode**: Reduce token usage by ~60% for cost savings
4. **Retry Logic**: 3 attempts with exponential backoff
5. **Sanity Checking**: Compare against Simple strategy baseline
6. **Confidence Scores**: AI returns confidence (0-100%) for each recommendation
7. **Reasoning**: Human-readable explanation for each recommendation
8. **Min/Max Constraints**: Enforce safety bounds (CPU: 0.01-16 cores, Memory: 100Mi-64Gi)
9. **HPA Awareness**: Conservative limits when HPA is configured
10. **OOMKill Detection**: Prioritize memory when OOM kills detected
11. **Full Test Coverage**: 19 tests covering all functionality

### ðŸ”§ Configuration

**Environment Variables:**
- `OPENAI_API_KEY`: OpenAI API key (auto-detected)
- `GEMINI_API_KEY`: Google Gemini API key (auto-detected)
- `ANTHROPIC_API_KEY`: Anthropic Claude API key (auto-detected)
- No key needed for Ollama (local)

**CLI Flags:**
```bash
--ai-provider          # openai/gemini/anthropic/ollama
--ai-model             # Model name (e.g., gpt-4)
--ai-api-key           # API key (overrides env var)
--ai-temperature       # 0-2 (default: 0.3)
--ai-max-tokens        # Max response tokens (default: 2000)
--ai-compact-mode      # Reduce token usage
--ai-exclude-simple-reference  # Exclude Simple strategy baseline (default: included)
--ai-timeout           # API timeout seconds (default: 60)
```

## Usage Examples

### Basic Usage

```bash
# Auto-detect provider from environment
export OPENAI_API_KEY="sk-..."
krr ai-assisted --namespace production

# Explicit provider
krr ai-assisted --ai-provider gemini --ai-model gemini-pro

# Compact mode for cost savings
krr ai-assisted --ai-compact-mode
```

### Local Inference with Ollama

```bash
# Start Ollama
ollama serve

# Pull a model
ollama pull llama3

# Run KRR
krr ai-assisted --ai-provider ollama --ai-model llama3
```

### Output Formats

```bash
# JSON for automation
krr ai-assisted -f json > recommendations.json

# CSV for spreadsheets
krr ai-assisted -f csv --fileoutput recommendations.csv

# Table for human review
krr ai-assisted -f table
```

## Testing

Run the AI strategy tests:
```bash
# All AI strategy tests
pytest tests/test_ai_strategy.py -v

# Specific test class
pytest tests/test_ai_strategy.py::TestProviderIntegration -v

# All tests including AI
pytest tests/ -v
```

**Test Coverage:**
- Stats extraction (4 tests)
- Prompt formatting (4 tests)
- Provider integration (3 tests)
- Auto-detection (4 tests)
- Validation (1 test)
- Output format (1 test)
- Error handling (2 tests)

## Design Decisions

### 1. Why `requests` instead of official SDKs?

**Pros:**
- Single lightweight dependency
- Consistent interface across all providers
- No version conflicts between provider SDKs
- Easier to add new providers
- Full control over HTTP requests

**Cons:**
- No automatic retries from SDKs (we implement our own)
- No built-in rate limiting (providers handle this)

### 2. Why numpy instead of sklearn?

**Pros:**
- Already a dependency of KRR
- Sufficient for simple linear regression
- Lightweight and fast
- `np.polyfit(deg=1)` provides slope for trend analysis

**Cons:**
- Less sophisticated than sklearn's LinearRegression
- No built-in feature scaling

### 3. Why separate prompt file?

**Pros:**
- Clear separation of concerns
- Easier to test prompt generation
- Simpler to update prompts without touching strategy logic
- Better readability

**Cons:**
- Extra import

### 4. Why compact mode?

Token costs can add up with many workloads:
- Full mode: ~1500-2000 tokens per workload
- Compact mode: ~600-800 tokens per workload

For 1000 workloads:
- Full: ~1.8M tokens
- Compact: ~700K tokens (61% savings)

## Performance Considerations

### Token Usage

**Full Mode (per workload):**
- System prompt: ~800 tokens
- User prompt: ~700-1200 tokens (depends on pod count)
- Response: ~200-300 tokens
- **Total: ~1700-2300 tokens**

**Compact Mode:**
- System prompt: ~800 tokens (same)
- User prompt: ~300-500 tokens (compressed)
- Response: ~200-300 tokens (same)
- **Total: ~1300-1600 tokens (38% reduction)**

### API Latency

Average response times:
- OpenAI GPT-4: 3-5 seconds
- OpenAI GPT-3.5: 1-2 seconds
- Gemini Pro: 2-4 seconds
- Anthropic Claude: 2-4 seconds
- Ollama (local): 5-15 seconds (depends on hardware)

With `--max-workers 10` (default), can process ~120-600 workloads/minute.

## Cost Estimates

**Per 100 workloads (compact mode):**

| Provider | Model | Input | Output | Total |
|----------|-------|-------|--------|-------|
| OpenAI | GPT-4 Turbo | $0.21 | $0.06 | **$0.27** |
| OpenAI | GPT-3.5 Turbo | $0.0021 | $0.0006 | **$0.0027** |
| Gemini | gemini-pro | Free | Free | **$0** |
| Anthropic | claude-3-sonnet | $0.09 | $0.045 | **$0.135** |
| Ollama | llama3 | Local | Local | **$0** |

## Troubleshooting

### API Key Not Found

```
ValueError: No AI provider API key found
```

**Solution:** Set environment variable:
```bash
export OPENAI_API_KEY="your-key"
```

### Provider Detection Failed

```
ValueError: No AI provider could be detected
```

**Solution:** Explicitly specify provider:
```bash
krr ai-assisted --ai-provider openai
```

### Low Confidence Scores

```
AI: Insufficient data... (conf: 25%)
```

**Solutions:**
- Increase `--history-duration` to gather more data
- Ensure Prometheus has historical metrics
- Check that workloads have been running long enough

### Rate Limiting

```
HTTP 429: Rate limit exceeded
```

**Solutions:**
- Add delays between runs
- Use `--max-workers 1` to serialize requests
- Upgrade API tier

## Future Enhancements

Potential improvements:
1. **Fine-tuning**: Train models on successful recommendation patterns
2. **Multi-metric analysis**: Consider network, disk I/O
3. **Seasonality detection**: Weekly/daily patterns
4. **Cost awareness**: Factor in node costs and bin packing
5. **Cluster-wide optimization**: Consider resource fragmentation
6. **Learning from outcomes**: Track recommendation effectiveness
7. **Recommendation explanation**: More detailed reasoning
8. **Interactive mode**: Ask clarifying questions
9. **Custom constraints**: Per-namespace or per-workload rules
10. **Batch optimization**: Optimize entire namespace together

## Contributing

When adding features:
1. Update tests in `tests/test_ai_strategy.py`
2. Update documentation in `docs/ai-assisted-strategy.md`
3. Add examples to this README
4. Ensure all 94 tests pass: `pytest tests/ -v`

## References

- [KRR Main Documentation](../../README.md)
- [AI Strategy Guide](../../docs/ai-assisted-strategy.md)
- [Simple Strategy Implementation](./simple.py)
- [Strategy Pattern Architecture](../core/abstract/strategies.py)
